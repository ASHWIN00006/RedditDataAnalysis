"""reddit_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oOHfOOiM_YyYHjbDrc7lsy2gf3ANDtk0
"""

!pip install asyncpraw

# DataExtractionFromRoosterteethSubreddit
import nest_asyncio
import asyncio
import asyncpraw
import pandas as pd
from datetime import datetime
from google.colab import userdata

# Apply nest_asyncio to avoid event loop issues
nest_asyncio.apply()

# Global Reddit instance
reddit = None

async def authenticate():
    """Authenticate Reddit API using stored credentials."""
    global reddit
    client_id = userdata.get("client_id")
    client_secret = userdata.get("client_secret")
    user_agent = userdata.get("user_agent")

    reddit = asyncpraw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent
    )
    print("âœ… Reddit API Authentication Ready!")
    return reddit

# Define the timeframe for data collection
start_date = datetime(2025, 1, 1).timestamp()
end_date = datetime(2025, 4, 15).timestamp()

async def gather_data(subreddit_name):
    """Gather submission (post) and comment data from the subreddit."""
    global reddit
    if reddit is None:
        reddit = await authenticate()

    print(f"ðŸ“¥ Collecting data from r/{subreddit_name} (01 Jan 2025 - 15 Apr 2025)...")

    try:
        subreddit = await reddit.subreddit(subreddit_name)
        data = []

        async for submission in subreddit.new(limit=1000):
            if not (start_date <= submission.created_utc <= end_date):
                continue  # Skip posts outside the date range

            try:
                await submission.load()
            except Exception:
                continue

            # ðŸ”¹ Add Submission Data (Each Submission is now included!)
            data.append({
                "subreddit": subreddit_name,
                "submission_id": submission.id,
                "parent_id": submission.id,  # Submission is its own parent
                "target_author": "",
                "comment_id": submission.id,  # Submission_id = comment_id for posts
                "source_author": submission.author.name if submission.author else "[deleted]",
                "created_utc": datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                "comment_score": "",  # No comment score for submissions
                "post_score": submission.score,
                "text": submission.selftext if submission.selftext else "",
                "title": submission.title
            })

            # Fetch and process comments
            try:
                await submission.comments.replace_more(limit=0)
            except Exception:
                continue

            comments = getattr(submission.comments, 'list', lambda: [])() or []
            for comment in comments:
                if not hasattr(comment, "body") or comment.body is None:
                    continue

                # Determine Parent-Child Relationship
                if comment.parent_id.startswith("t3_"):  # Top-Level Comment
                    target_author = submission.author.name if submission.author else "[deleted]"
                else:  # Nested Comment
                    try:
                        parent_comment = await reddit.comment(comment.parent_id.split("_")[1])
                        target_author = parent_comment.author.name if parent_comment.author else "[deleted]"
                    except Exception:
                        target_author = "[parent comment deleted]"

                # ðŸ”¹ Add Comment Data (Now Includes Parent-Child Logic!)
                data.append({
                    "subreddit": subreddit_name,
                    "submission_id": submission.id,
                    "parent_id": comment.parent_id.split("_")[1],  # Store actual ID
                    "target_author": target_author,
                    "comment_id": comment.id,
                    "source_author": comment.author.name if comment.author else "[deleted]",
                    "created_utc": datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                    "comment_score": comment.score,
                    "post_score": submission.score,
                    "text": comment.body,
                    "title": submission.title
                })

            await asyncio.sleep(2)  # Avoid API rate limits

    except Exception as e:
        print(f"âŒ Error gathering data from r/{subreddit_name}: {e}")
        import traceback
        traceback.print_exc()

    await save_data(data, subreddit_name)

async def save_data(data, subreddit_name):
    """Save collected data to a CSV file."""
    if not data:
        print(f"âš ï¸ No data collected from r/{subreddit_name}")
        return

    df = pd.DataFrame(data)
    filename = f"{subreddit_name}_JanApr2025_cleaned.csv"
    df.to_csv(filename, index=False)
    print(f"âœ… Data saved to {filename}")

async def main():
    await gather_data("roosterteeth")
    if reddit:
        await reddit.close()

if __name__ == "__main__":
    asyncio.run(main())

#DataExtractionFromredvsblueSubreddit
import nest_asyncio
import asyncio
import asyncpraw
import pandas as pd
from datetime import datetime
from google.colab import userdata

# Apply nest_asyncio to avoid event loop issues
nest_asyncio.apply()

# Global Reddit instance
reddit = None

async def authenticate():
    """Authenticate Reddit API using stored credentials."""
    global reddit
    client_id = userdata.get("client_id")
    client_secret = userdata.get("client_secret")
    user_agent = userdata.get("user_agent")

    reddit = asyncpraw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent
    )
    print("âœ… Reddit API Authentication Ready!")
    return reddit

# Define the timeframe for data collection
start_date = datetime(2025, 1, 1).timestamp()
end_date = datetime(2025, 4, 15).timestamp()

async def gather_data(subreddit_name):
    """Gather submission (post) and comment data from the subreddit."""
    global reddit
    if reddit is None:
        reddit = await authenticate()

    print(f"ðŸ“¥ Collecting data from r/{subreddit_name} (01 Jan 2025 - 15 Apr 2025)...")

    try:
        subreddit = await reddit.subreddit(subreddit_name)
        data = []

        async for submission in subreddit.new(limit=1000):
            if not (start_date <= submission.created_utc <= end_date):
                continue  # Skip posts outside the date range

            try:
                await submission.load()
            except Exception:
                continue

            # ðŸ”¹ Add Submission Data (Each Submission is now included!)
            data.append({
                "subreddit": subreddit_name,
                "submission_id": submission.id,
                "parent_id": submission.id,  # Submission is its own parent
                "target_author": "",
                "comment_id": submission.id,  # Submission_id = comment_id for posts
                "source_author": submission.author.name if submission.author else "[deleted]",
                "text": submission.selftext if submission.selftext else "",
                "title": submission.title,
                "comment_score": "",  # No comment score for submissions
                "created_utc": datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                "date": datetime.utcfromtimestamp(submission.created_utc).strftime('%y.%m.%d'),
                "time": datetime.utcfromtimestamp(submission.created_utc).strftime('%H:%M:%S')

            })

            # Fetch and process comments
            try:
                await submission.comments.replace_more(limit=0)
            except Exception:
                continue

            comments = getattr(submission.comments, 'list', lambda: [])() or []
            for comment in comments:
                if not hasattr(comment, "body") or comment.body is None:
                    continue

                # Determine Parent-Child Relationship
                if comment.parent_id.startswith("t3_"):  # Top-Level Comment
                    target_author = submission.author.name if submission.author else "[deleted]"
                else:  # Nested Comment
                    try:
                        parent_comment = await reddit.comment(comment.parent_id.split("_")[1])
                        target_author = parent_comment.author.name if parent_comment.author else "[deleted]"
                    except Exception:
                        target_author = "[parent comment deleted]"

                # ðŸ”¹ Add Comment Data (Now Includes Parent-Child Logic!)
                data.append({
                    "subreddit": subreddit_name,
                    "submission_id": submission.id,
                    "parent_id": comment.parent_id.split("_")[1],  # Store actual ID
                    "target_author": target_author,
                    "comment_id": comment.id,
                    "source_author": comment.author.name if comment.author else "[deleted]",
                    "text": comment.body,
                    "title": submission.title,
                    "comment_score": comment.score,
                    "created_utc": datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),
                    "date": datetime.utcfromtimestamp(comment.created_utc).strftime('%y.%m.%d'),
                    "time": datetime.utcfromtimestamp(comment.created_utc).strftime('%H:%M:%S')

                })

            await asyncio.sleep(2)  # Avoid API rate limits

    except Exception as e:
        print(f"âŒ Error gathering data from r/{subreddit_name}: {e}")
        import traceback
        traceback.print_exc()

    await save_data(data, subreddit_name)

async def save_data(data, subreddit_name):
    """Save collected data to a CSV file."""
    if not data:
        print(f"âš ï¸ No data collected from r/{subreddit_name}")
        return

    df = pd.DataFrame(data)
    filename = f"{subreddit_name}_JanApr025_cleaned.csv"
    df.to_csv(filename, index=False)
    print(f"âœ… Data saved to {filename}")

async def main():
    await gather_data("redvsblue")
    if reddit:
        await reddit.close()

if __name__ == "__main__":
    asyncio.run(main())

#InDegreeVOutDegreeDistributionPlot
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Loading Excel file and  the relevant sheet
file_path = 'further_analysis.xlsx'
df = pd.read_excel(file_path, sheet_name='Sheet2')

# Creating scatter plot
plt.figure(figsize=(7, 5))
sns.scatterplot(
    data=df,
    x='In-degree',
    y='Out-degree',
    palette=palette,
    s=60
)

plt.title('In-degree vs Out-degree ')
plt.xlabel('In-degree')
plt.ylabel('Out-degree')
plt.grid(True)
plt.tight_layout()
plt.show()

#InDegreeVOutDegreeColoredBySentimentCategory
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Loading Excel file and the relevant sheet
file_path = 'further_analysis.xlsx'
df = pd.read_excel(file_path, sheet_name='Sheet2')

# Categorizing sentiment score
def categorize_sentiment(score):
    if score < 0.05:
        return 'Bad Sentiment Score'
    elif score > 0.12:
        return 'Good Sentiment Score'
    else:
        return 'Neutral'

# Applying the categorization
df['Sentiment Category'] = df['Average sentiment of all posts'].apply(categorize_sentiment)

# Setting up color palette
palette = {
    'Bad Sentiment Score': 'red',
    'Neutral': 'black',
    'Good Sentiment Score': 'blue'
}

# Creating scatter plot
plt.figure(figsize=(7, 5))
sns.scatterplot(
    data=df,
    x='In-degree',
    y='Out-degree',
    hue='Sentiment Category',
    palette=palette,
    s=60
)

plt.title('In-degree vs Out-degree Colored by Sentiment Category')
plt.xlabel('In-degree')
plt.ylabel('Out-degree')
plt.legend(title='Sentiment Category')
plt.grid(True)
plt.tight_layout()
plt.show()

#AverageSentimentOverTime(With and Without Toxic Users)
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Loading the Excel file and the relevant sheet
file_path = 'further_analysis.xlsx'
df = pd.read_excel(file_path, sheet_name='raw_data')

# Converting 'date' column to datetime format
df['date'] = pd.to_datetime(df['date'], format='%y.%m.%d', errors='coerce')

# List of toxic users
toxic_users = [
    'Startan117', 'RegulationSuperFan', 'FragrantHockeyFan', 'GalacticMe99',
    'Impossible-Pie4849', 'specslota', 'cocachair', 'CodasWanderer',
    '_TheTurtleBox_', 'McRaeWritescom'
]
#Grouping users
# Group 1: All users
all_users_sentiment = (
    df.groupby('date')['sentiment_score'].mean()
    .sort_index()
)

# Group 2: Excluding toxic users
filtered_df = df[~df['source_author'].isin(toxic_users)]
non_toxic_sentiment = (
    filtered_df.groupby('date')['sentiment_score'].mean()
    .sort_index()
)

# Plotting both time series
plt.figure(figsize=(7, 5))
plt.plot(all_users_sentiment.index, all_users_sentiment.values, marker='o', label='Including Toxic Users')
plt.plot(non_toxic_sentiment.index, non_toxic_sentiment.values, marker='o', label='Excluding Toxic Users')

# Formatting the x-axis for dates
plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

plt.title("Average Sentiment Over Time (With and Without Toxic Users)")
plt.xlabel("Date")
plt.ylabel("Average Sentiment Score")
plt.xticks(rotation=45)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
